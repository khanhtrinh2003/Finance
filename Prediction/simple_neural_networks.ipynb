{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "stock = \"CVX\"\n",
    "df = pd.DataFrame(yf.download(stock, '2010-01-01', '2022-12-31')[\"Close\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling(lag, data):\n",
    "    A = []\n",
    "    df = data.tolist()\n",
    "    for i in range(lag, len(df)):\n",
    "        A.append(df[i-lag:i]) \n",
    "    return pd.DataFrame(A)\n",
    "\n",
    "def UoD(x):\n",
    "    if x<0:\n",
    "        return -1\n",
    "    if x>0:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "\n",
    "def NN(x,y, input_dim, out_dim):\n",
    "\t\tmodel = Sequential()\n",
    "\t\tmodel.add(Dense(100, input_dim = input_dim, activation=\"sigmoid\"))\n",
    "\t\tmodel.add(Dense(100, input_dim = 100, activation=\"sigmoid\"))\n",
    "\t\tmodel.add(Dense(100, input_dim = 100, activation=\"sigmoid\"))\n",
    "\t\tmodel.add(Dense(out_dim, input_dim = 5, activation=\"softmax\"))\n",
    "\n",
    "\t\toptimizer=Adam()\n",
    "\t\tmodel.compile(optimizer=optimizer, loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "\t\tmodel.fit(x, y, epochs=1000, verbose=2)\n",
    "\t\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"returns\"] =df[\"Close\"].pct_change()\n",
    "df[\"UoD\"]= df[\"returns\"].map(UoD)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "2010-01-05    1.0\n",
       "2010-01-06    1.0\n",
       "2010-01-07   -1.0\n",
       "2010-01-08   -1.0\n",
       "2010-01-11   -1.0\n",
       "             ... \n",
       "2022-12-20    1.0\n",
       "2022-12-21    1.0\n",
       "2022-12-22   -1.0\n",
       "2022-12-23    1.0\n",
       "2022-12-27    1.0\n",
       "Name: UoD, Length: 3268, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"UoD\"].shift(-3)[:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "data = (scaler.fit_transform(df[\"Close\"].values.reshape(-1,1)))\n",
    "df[\"p\"]=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>returns</th>\n",
       "      <th>UoD</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>79.629997</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>1</td>\n",
       "      <td>0.189868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>79.330002</td>\n",
       "      <td>-0.003767</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.187626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-08</th>\n",
       "      <td>79.470001</td>\n",
       "      <td>0.001765</td>\n",
       "      <td>1</td>\n",
       "      <td>0.188672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-11</th>\n",
       "      <td>80.879997</td>\n",
       "      <td>0.017742</td>\n",
       "      <td>1</td>\n",
       "      <td>0.199208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-12</th>\n",
       "      <td>80.410004</td>\n",
       "      <td>-0.005811</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.195696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-23</th>\n",
       "      <td>177.399994</td>\n",
       "      <td>0.030916</td>\n",
       "      <td>1</td>\n",
       "      <td>0.920421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-27</th>\n",
       "      <td>179.630005</td>\n",
       "      <td>0.012571</td>\n",
       "      <td>1</td>\n",
       "      <td>0.937084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-28</th>\n",
       "      <td>176.979996</td>\n",
       "      <td>-0.014753</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.917283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-29</th>\n",
       "      <td>178.320007</td>\n",
       "      <td>0.007572</td>\n",
       "      <td>1</td>\n",
       "      <td>0.927296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-30</th>\n",
       "      <td>179.490005</td>\n",
       "      <td>0.006561</td>\n",
       "      <td>1</td>\n",
       "      <td>0.936038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3270 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Close   returns  UoD         p\n",
       "Date                                           \n",
       "2010-01-06   79.629997  0.000126    1  0.189868\n",
       "2010-01-07   79.330002 -0.003767   -1  0.187626\n",
       "2010-01-08   79.470001  0.001765    1  0.188672\n",
       "2010-01-11   80.879997  0.017742    1  0.199208\n",
       "2010-01-12   80.410004 -0.005811   -1  0.195696\n",
       "...                ...       ...  ...       ...\n",
       "2022-12-23  177.399994  0.030916    1  0.920421\n",
       "2022-12-27  179.630005  0.012571    1  0.937084\n",
       "2022-12-28  176.979996 -0.014753   -1  0.917283\n",
       "2022-12-29  178.320007  0.007572    1  0.927296\n",
       "2022-12-30  179.490005  0.006561    1  0.936038\n",
       "\n",
       "[3270 rows x 4 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag = 20\n",
    "A = rolling(lag, df[\"p\"])\n",
    "b = df[\"UoD\"].shift(-lag)[:-lag]\n",
    "\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "y = encoder.fit_transform(np.array(b).reshape(-1, 1)).toarray()\n",
    "deon, dete, keyon, keyte = train_test_split(A.values,y, test_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "102/102 - 1s - loss: 0.7425 - accuracy: 0.5112 - 640ms/epoch - 6ms/step\n",
      "Epoch 2/1000\n",
      "102/102 - 0s - loss: 0.7215 - accuracy: 0.4971 - 290ms/epoch - 3ms/step\n",
      "Epoch 3/1000\n",
      "102/102 - 0s - loss: 0.7184 - accuracy: 0.5011 - 190ms/epoch - 2ms/step\n",
      "Epoch 4/1000\n",
      "102/102 - 0s - loss: 0.7163 - accuracy: 0.5091 - 183ms/epoch - 2ms/step\n",
      "Epoch 5/1000\n",
      "102/102 - 0s - loss: 0.7200 - accuracy: 0.5011 - 167ms/epoch - 2ms/step\n",
      "Epoch 6/1000\n",
      "102/102 - 0s - loss: 0.7183 - accuracy: 0.4891 - 153ms/epoch - 2ms/step\n",
      "Epoch 7/1000\n",
      "102/102 - 0s - loss: 0.7174 - accuracy: 0.5045 - 153ms/epoch - 2ms/step\n",
      "Epoch 8/1000\n",
      "102/102 - 0s - loss: 0.7221 - accuracy: 0.4995 - 157ms/epoch - 2ms/step\n",
      "Epoch 9/1000\n",
      "102/102 - 0s - loss: 0.7180 - accuracy: 0.4955 - 223ms/epoch - 2ms/step\n",
      "Epoch 10/1000\n",
      "102/102 - 0s - loss: 0.7210 - accuracy: 0.5029 - 241ms/epoch - 2ms/step\n",
      "Epoch 11/1000\n",
      "102/102 - 0s - loss: 0.7174 - accuracy: 0.5045 - 143ms/epoch - 1ms/step\n",
      "Epoch 12/1000\n",
      "102/102 - 0s - loss: 0.7157 - accuracy: 0.5192 - 146ms/epoch - 1ms/step\n",
      "Epoch 13/1000\n",
      "102/102 - 0s - loss: 0.7144 - accuracy: 0.5235 - 151ms/epoch - 1ms/step\n",
      "Epoch 14/1000\n",
      "102/102 - 0s - loss: 0.7164 - accuracy: 0.5128 - 158ms/epoch - 2ms/step\n",
      "Epoch 15/1000\n",
      "102/102 - 0s - loss: 0.7169 - accuracy: 0.5060 - 163ms/epoch - 2ms/step\n",
      "Epoch 16/1000\n",
      "102/102 - 0s - loss: 0.7195 - accuracy: 0.5014 - 203ms/epoch - 2ms/step\n",
      "Epoch 17/1000\n",
      "102/102 - 0s - loss: 0.7175 - accuracy: 0.5020 - 214ms/epoch - 2ms/step\n",
      "Epoch 18/1000\n",
      "102/102 - 0s - loss: 0.7164 - accuracy: 0.5032 - 159ms/epoch - 2ms/step\n",
      "Epoch 19/1000\n",
      "102/102 - 0s - loss: 0.7173 - accuracy: 0.5072 - 148ms/epoch - 1ms/step\n",
      "Epoch 20/1000\n",
      "102/102 - 0s - loss: 0.7173 - accuracy: 0.5118 - 152ms/epoch - 1ms/step\n",
      "Epoch 21/1000\n",
      "102/102 - 0s - loss: 0.7182 - accuracy: 0.5008 - 200ms/epoch - 2ms/step\n",
      "Epoch 22/1000\n",
      "102/102 - 0s - loss: 0.7192 - accuracy: 0.4900 - 174ms/epoch - 2ms/step\n",
      "Epoch 23/1000\n",
      "102/102 - 0s - loss: 0.7182 - accuracy: 0.5085 - 157ms/epoch - 2ms/step\n",
      "Epoch 24/1000\n",
      "102/102 - 0s - loss: 0.7167 - accuracy: 0.5035 - 149ms/epoch - 1ms/step\n",
      "Epoch 25/1000\n",
      "102/102 - 0s - loss: 0.7172 - accuracy: 0.5103 - 158ms/epoch - 2ms/step\n",
      "Epoch 26/1000\n",
      "102/102 - 0s - loss: 0.7180 - accuracy: 0.5042 - 156ms/epoch - 2ms/step\n",
      "Epoch 27/1000\n",
      "102/102 - 0s - loss: 0.7176 - accuracy: 0.5112 - 155ms/epoch - 2ms/step\n",
      "Epoch 28/1000\n",
      "102/102 - 0s - loss: 0.7166 - accuracy: 0.5078 - 157ms/epoch - 2ms/step\n",
      "Epoch 29/1000\n",
      "102/102 - 0s - loss: 0.7158 - accuracy: 0.5131 - 160ms/epoch - 2ms/step\n",
      "Epoch 30/1000\n",
      "102/102 - 0s - loss: 0.7164 - accuracy: 0.5054 - 159ms/epoch - 2ms/step\n",
      "Epoch 31/1000\n",
      "102/102 - 0s - loss: 0.7166 - accuracy: 0.5106 - 160ms/epoch - 2ms/step\n",
      "Epoch 32/1000\n",
      "102/102 - 0s - loss: 0.7194 - accuracy: 0.4955 - 158ms/epoch - 2ms/step\n",
      "Epoch 33/1000\n",
      "102/102 - 0s - loss: 0.7171 - accuracy: 0.4977 - 160ms/epoch - 2ms/step\n",
      "Epoch 34/1000\n",
      "102/102 - 0s - loss: 0.7171 - accuracy: 0.5057 - 161ms/epoch - 2ms/step\n",
      "Epoch 35/1000\n",
      "102/102 - 0s - loss: 0.7151 - accuracy: 0.5239 - 157ms/epoch - 2ms/step\n",
      "Epoch 36/1000\n",
      "102/102 - 0s - loss: 0.7171 - accuracy: 0.5042 - 157ms/epoch - 2ms/step\n",
      "Epoch 37/1000\n",
      "102/102 - 0s - loss: 0.7162 - accuracy: 0.5057 - 158ms/epoch - 2ms/step\n",
      "Epoch 38/1000\n",
      "102/102 - 0s - loss: 0.7167 - accuracy: 0.5202 - 159ms/epoch - 2ms/step\n",
      "Epoch 39/1000\n",
      "102/102 - 0s - loss: 0.7170 - accuracy: 0.5217 - 157ms/epoch - 2ms/step\n",
      "Epoch 40/1000\n",
      "102/102 - 0s - loss: 0.7154 - accuracy: 0.5078 - 157ms/epoch - 2ms/step\n",
      "Epoch 41/1000\n",
      "102/102 - 0s - loss: 0.7162 - accuracy: 0.5038 - 153ms/epoch - 1ms/step\n",
      "Epoch 42/1000\n",
      "102/102 - 0s - loss: 0.7169 - accuracy: 0.5057 - 159ms/epoch - 2ms/step\n",
      "Epoch 43/1000\n",
      "102/102 - 0s - loss: 0.7164 - accuracy: 0.5159 - 197ms/epoch - 2ms/step\n",
      "Epoch 44/1000\n",
      "102/102 - 0s - loss: 0.7178 - accuracy: 0.5023 - 149ms/epoch - 1ms/step\n",
      "Epoch 45/1000\n",
      "102/102 - 0s - loss: 0.7178 - accuracy: 0.5032 - 157ms/epoch - 2ms/step\n",
      "Epoch 46/1000\n",
      "102/102 - 0s - loss: 0.7159 - accuracy: 0.5088 - 158ms/epoch - 2ms/step\n",
      "Epoch 47/1000\n",
      "102/102 - 0s - loss: 0.7173 - accuracy: 0.5069 - 156ms/epoch - 2ms/step\n",
      "Epoch 48/1000\n",
      "102/102 - 0s - loss: 0.7161 - accuracy: 0.5072 - 154ms/epoch - 2ms/step\n",
      "Epoch 49/1000\n",
      "102/102 - 0s - loss: 0.7169 - accuracy: 0.5035 - 157ms/epoch - 2ms/step\n",
      "Epoch 50/1000\n",
      "102/102 - 0s - loss: 0.7170 - accuracy: 0.5097 - 158ms/epoch - 2ms/step\n",
      "Epoch 51/1000\n",
      "102/102 - 0s - loss: 0.7163 - accuracy: 0.5069 - 160ms/epoch - 2ms/step\n",
      "Epoch 52/1000\n",
      "102/102 - 0s - loss: 0.7181 - accuracy: 0.4940 - 162ms/epoch - 2ms/step\n",
      "Epoch 53/1000\n",
      "102/102 - 0s - loss: 0.7153 - accuracy: 0.5128 - 162ms/epoch - 2ms/step\n",
      "Epoch 54/1000\n",
      "102/102 - 0s - loss: 0.7157 - accuracy: 0.5032 - 169ms/epoch - 2ms/step\n",
      "Epoch 55/1000\n",
      "102/102 - 0s - loss: 0.7168 - accuracy: 0.5134 - 159ms/epoch - 2ms/step\n",
      "Epoch 56/1000\n",
      "102/102 - 0s - loss: 0.7185 - accuracy: 0.4897 - 158ms/epoch - 2ms/step\n",
      "Epoch 57/1000\n",
      "102/102 - 0s - loss: 0.7156 - accuracy: 0.5165 - 159ms/epoch - 2ms/step\n",
      "Epoch 58/1000\n",
      "102/102 - 0s - loss: 0.7176 - accuracy: 0.5100 - 157ms/epoch - 2ms/step\n",
      "Epoch 59/1000\n",
      "102/102 - 0s - loss: 0.7156 - accuracy: 0.5069 - 159ms/epoch - 2ms/step\n",
      "Epoch 60/1000\n",
      "102/102 - 0s - loss: 0.7162 - accuracy: 0.5020 - 156ms/epoch - 2ms/step\n",
      "Epoch 61/1000\n",
      "102/102 - 0s - loss: 0.7170 - accuracy: 0.5023 - 155ms/epoch - 2ms/step\n",
      "Epoch 62/1000\n",
      "102/102 - 0s - loss: 0.7157 - accuracy: 0.5155 - 162ms/epoch - 2ms/step\n",
      "Epoch 63/1000\n",
      "102/102 - 0s - loss: 0.7160 - accuracy: 0.5177 - 164ms/epoch - 2ms/step\n",
      "Epoch 64/1000\n",
      "102/102 - 0s - loss: 0.7151 - accuracy: 0.5217 - 162ms/epoch - 2ms/step\n",
      "Epoch 65/1000\n",
      "102/102 - 0s - loss: 0.7150 - accuracy: 0.5251 - 165ms/epoch - 2ms/step\n",
      "Epoch 66/1000\n",
      "102/102 - 0s - loss: 0.7169 - accuracy: 0.5103 - 168ms/epoch - 2ms/step\n",
      "Epoch 67/1000\n",
      "102/102 - 0s - loss: 0.7166 - accuracy: 0.4937 - 161ms/epoch - 2ms/step\n",
      "Epoch 68/1000\n",
      "102/102 - 0s - loss: 0.7156 - accuracy: 0.5125 - 165ms/epoch - 2ms/step\n",
      "Epoch 69/1000\n",
      "102/102 - 0s - loss: 0.7187 - accuracy: 0.4998 - 170ms/epoch - 2ms/step\n",
      "Epoch 70/1000\n",
      "102/102 - 0s - loss: 0.7160 - accuracy: 0.5112 - 163ms/epoch - 2ms/step\n",
      "Epoch 71/1000\n",
      "102/102 - 0s - loss: 0.7155 - accuracy: 0.5094 - 171ms/epoch - 2ms/step\n",
      "Epoch 72/1000\n",
      "102/102 - 0s - loss: 0.7159 - accuracy: 0.5069 - 161ms/epoch - 2ms/step\n",
      "Epoch 73/1000\n",
      "102/102 - 0s - loss: 0.7154 - accuracy: 0.5023 - 166ms/epoch - 2ms/step\n",
      "Epoch 74/1000\n",
      "102/102 - 0s - loss: 0.7166 - accuracy: 0.5257 - 166ms/epoch - 2ms/step\n",
      "Epoch 75/1000\n",
      "102/102 - 0s - loss: 0.7154 - accuracy: 0.5177 - 170ms/epoch - 2ms/step\n",
      "Epoch 76/1000\n",
      "102/102 - 0s - loss: 0.7159 - accuracy: 0.5152 - 158ms/epoch - 2ms/step\n",
      "Epoch 77/1000\n",
      "102/102 - 0s - loss: 0.7168 - accuracy: 0.5020 - 162ms/epoch - 2ms/step\n",
      "Epoch 78/1000\n",
      "102/102 - 0s - loss: 0.7162 - accuracy: 0.5078 - 170ms/epoch - 2ms/step\n",
      "Epoch 79/1000\n",
      "102/102 - 0s - loss: 0.7158 - accuracy: 0.5091 - 166ms/epoch - 2ms/step\n",
      "Epoch 80/1000\n",
      "102/102 - 0s - loss: 0.7152 - accuracy: 0.5168 - 210ms/epoch - 2ms/step\n",
      "Epoch 81/1000\n",
      "102/102 - 0s - loss: 0.7166 - accuracy: 0.5054 - 150ms/epoch - 1ms/step\n",
      "Epoch 82/1000\n",
      "102/102 - 0s - loss: 0.7161 - accuracy: 0.5051 - 153ms/epoch - 2ms/step\n",
      "Epoch 83/1000\n",
      "102/102 - 0s - loss: 0.7162 - accuracy: 0.5014 - 147ms/epoch - 1ms/step\n",
      "Epoch 84/1000\n",
      "102/102 - 0s - loss: 0.7161 - accuracy: 0.5026 - 157ms/epoch - 2ms/step\n",
      "Epoch 85/1000\n",
      "102/102 - 0s - loss: 0.7154 - accuracy: 0.5069 - 154ms/epoch - 2ms/step\n",
      "Epoch 86/1000\n",
      "102/102 - 0s - loss: 0.7155 - accuracy: 0.5245 - 158ms/epoch - 2ms/step\n",
      "Epoch 87/1000\n",
      "102/102 - 0s - loss: 0.7167 - accuracy: 0.5017 - 164ms/epoch - 2ms/step\n",
      "Epoch 88/1000\n",
      "102/102 - 0s - loss: 0.7165 - accuracy: 0.5128 - 163ms/epoch - 2ms/step\n",
      "Epoch 89/1000\n",
      "102/102 - 0s - loss: 0.7157 - accuracy: 0.5020 - 165ms/epoch - 2ms/step\n",
      "Epoch 90/1000\n",
      "102/102 - 0s - loss: 0.7160 - accuracy: 0.5140 - 157ms/epoch - 2ms/step\n",
      "Epoch 91/1000\n",
      "102/102 - 0s - loss: 0.7164 - accuracy: 0.5005 - 158ms/epoch - 2ms/step\n",
      "Epoch 92/1000\n",
      "102/102 - 0s - loss: 0.7171 - accuracy: 0.4974 - 155ms/epoch - 2ms/step\n",
      "Epoch 93/1000\n",
      "102/102 - 0s - loss: 0.7175 - accuracy: 0.4989 - 165ms/epoch - 2ms/step\n",
      "Epoch 94/1000\n",
      "102/102 - 0s - loss: 0.7177 - accuracy: 0.4922 - 152ms/epoch - 1ms/step\n",
      "Epoch 95/1000\n",
      "102/102 - 0s - loss: 0.7159 - accuracy: 0.5020 - 155ms/epoch - 2ms/step\n",
      "Epoch 96/1000\n",
      "102/102 - 0s - loss: 0.7161 - accuracy: 0.5205 - 156ms/epoch - 2ms/step\n",
      "Epoch 97/1000\n",
      "102/102 - 0s - loss: 0.7162 - accuracy: 0.5183 - 157ms/epoch - 2ms/step\n",
      "Epoch 98/1000\n",
      "102/102 - 0s - loss: 0.7157 - accuracy: 0.5085 - 166ms/epoch - 2ms/step\n",
      "Epoch 99/1000\n",
      "102/102 - 0s - loss: 0.7179 - accuracy: 0.5088 - 164ms/epoch - 2ms/step\n",
      "Epoch 100/1000\n",
      "102/102 - 0s - loss: 0.7163 - accuracy: 0.5029 - 169ms/epoch - 2ms/step\n",
      "Epoch 101/1000\n",
      "102/102 - 0s - loss: 0.7154 - accuracy: 0.5137 - 164ms/epoch - 2ms/step\n",
      "Epoch 102/1000\n",
      "102/102 - 0s - loss: 0.7146 - accuracy: 0.5214 - 165ms/epoch - 2ms/step\n",
      "Epoch 103/1000\n",
      "102/102 - 0s - loss: 0.7157 - accuracy: 0.5011 - 167ms/epoch - 2ms/step\n",
      "Epoch 104/1000\n",
      "102/102 - 0s - loss: 0.7162 - accuracy: 0.5106 - 167ms/epoch - 2ms/step\n",
      "Epoch 105/1000\n",
      "102/102 - 0s - loss: 0.7172 - accuracy: 0.4965 - 167ms/epoch - 2ms/step\n",
      "Epoch 106/1000\n",
      "102/102 - 0s - loss: 0.7160 - accuracy: 0.4955 - 167ms/epoch - 2ms/step\n",
      "Epoch 107/1000\n",
      "102/102 - 0s - loss: 0.7163 - accuracy: 0.5014 - 165ms/epoch - 2ms/step\n",
      "Epoch 108/1000\n",
      "102/102 - 0s - loss: 0.7173 - accuracy: 0.5078 - 165ms/epoch - 2ms/step\n",
      "Epoch 109/1000\n",
      "102/102 - 0s - loss: 0.7173 - accuracy: 0.4922 - 161ms/epoch - 2ms/step\n",
      "Epoch 110/1000\n",
      "102/102 - 0s - loss: 0.7180 - accuracy: 0.4863 - 163ms/epoch - 2ms/step\n",
      "Epoch 111/1000\n",
      "102/102 - 0s - loss: 0.7180 - accuracy: 0.4971 - 164ms/epoch - 2ms/step\n",
      "Epoch 112/1000\n",
      "102/102 - 0s - loss: 0.7171 - accuracy: 0.5100 - 163ms/epoch - 2ms/step\n",
      "Epoch 113/1000\n",
      "102/102 - 0s - loss: 0.7173 - accuracy: 0.5008 - 161ms/epoch - 2ms/step\n",
      "Epoch 114/1000\n",
      "102/102 - 0s - loss: 0.7159 - accuracy: 0.5085 - 163ms/epoch - 2ms/step\n",
      "Epoch 115/1000\n",
      "102/102 - 0s - loss: 0.7172 - accuracy: 0.4940 - 167ms/epoch - 2ms/step\n",
      "Epoch 116/1000\n",
      "102/102 - 0s - loss: 0.7161 - accuracy: 0.5112 - 183ms/epoch - 2ms/step\n",
      "Epoch 117/1000\n",
      "102/102 - 0s - loss: 0.7165 - accuracy: 0.5057 - 147ms/epoch - 1ms/step\n",
      "Epoch 118/1000\n",
      "102/102 - 0s - loss: 0.7167 - accuracy: 0.4937 - 153ms/epoch - 1ms/step\n",
      "Epoch 119/1000\n",
      "102/102 - 0s - loss: 0.7179 - accuracy: 0.4931 - 184ms/epoch - 2ms/step\n",
      "Epoch 120/1000\n",
      "102/102 - 0s - loss: 0.7154 - accuracy: 0.5134 - 145ms/epoch - 1ms/step\n",
      "Epoch 121/1000\n",
      "102/102 - 0s - loss: 0.7146 - accuracy: 0.5152 - 150ms/epoch - 1ms/step\n",
      "Epoch 122/1000\n",
      "102/102 - 0s - loss: 0.7160 - accuracy: 0.5042 - 188ms/epoch - 2ms/step\n",
      "Epoch 123/1000\n",
      "102/102 - 0s - loss: 0.7169 - accuracy: 0.4989 - 148ms/epoch - 1ms/step\n",
      "Epoch 124/1000\n",
      "102/102 - 0s - loss: 0.7161 - accuracy: 0.5082 - 154ms/epoch - 2ms/step\n",
      "Epoch 125/1000\n",
      "102/102 - 0s - loss: 0.7166 - accuracy: 0.4962 - 156ms/epoch - 2ms/step\n",
      "Epoch 126/1000\n",
      "102/102 - 0s - loss: 0.7152 - accuracy: 0.5128 - 162ms/epoch - 2ms/step\n",
      "Epoch 127/1000\n",
      "102/102 - 0s - loss: 0.7176 - accuracy: 0.4998 - 160ms/epoch - 2ms/step\n",
      "Epoch 128/1000\n",
      "102/102 - 0s - loss: 0.7164 - accuracy: 0.5063 - 157ms/epoch - 2ms/step\n",
      "Epoch 129/1000\n",
      "102/102 - 0s - loss: 0.7163 - accuracy: 0.4998 - 158ms/epoch - 2ms/step\n",
      "Epoch 130/1000\n",
      "102/102 - 0s - loss: 0.7160 - accuracy: 0.5085 - 161ms/epoch - 2ms/step\n",
      "Epoch 131/1000\n",
      "102/102 - 0s - loss: 0.7159 - accuracy: 0.5082 - 160ms/epoch - 2ms/step\n",
      "Epoch 132/1000\n",
      "102/102 - 0s - loss: 0.7170 - accuracy: 0.5020 - 158ms/epoch - 2ms/step\n",
      "Epoch 133/1000\n",
      "102/102 - 0s - loss: 0.7159 - accuracy: 0.5112 - 158ms/epoch - 2ms/step\n",
      "Epoch 134/1000\n",
      "102/102 - 0s - loss: 0.7161 - accuracy: 0.4940 - 155ms/epoch - 2ms/step\n",
      "Epoch 135/1000\n",
      "102/102 - 0s - loss: 0.7150 - accuracy: 0.5060 - 160ms/epoch - 2ms/step\n",
      "Epoch 136/1000\n",
      "102/102 - 0s - loss: 0.7175 - accuracy: 0.5038 - 160ms/epoch - 2ms/step\n",
      "Epoch 137/1000\n",
      "102/102 - 0s - loss: 0.7146 - accuracy: 0.5183 - 155ms/epoch - 2ms/step\n",
      "Epoch 138/1000\n",
      "102/102 - 0s - loss: 0.7155 - accuracy: 0.5171 - 152ms/epoch - 1ms/step\n",
      "Epoch 139/1000\n",
      "102/102 - 0s - loss: 0.7163 - accuracy: 0.5017 - 157ms/epoch - 2ms/step\n",
      "Epoch 140/1000\n",
      "102/102 - 0s - loss: 0.7151 - accuracy: 0.5106 - 156ms/epoch - 2ms/step\n",
      "Epoch 141/1000\n",
      "102/102 - 0s - loss: 0.7161 - accuracy: 0.5125 - 157ms/epoch - 2ms/step\n",
      "Epoch 142/1000\n",
      "102/102 - 0s - loss: 0.7158 - accuracy: 0.5051 - 172ms/epoch - 2ms/step\n",
      "Epoch 143/1000\n",
      "102/102 - 0s - loss: 0.7156 - accuracy: 0.5038 - 159ms/epoch - 2ms/step\n",
      "Epoch 144/1000\n",
      "102/102 - 0s - loss: 0.7158 - accuracy: 0.5131 - 159ms/epoch - 2ms/step\n",
      "Epoch 145/1000\n",
      "102/102 - 0s - loss: 0.7157 - accuracy: 0.5109 - 158ms/epoch - 2ms/step\n",
      "Epoch 146/1000\n",
      "102/102 - 0s - loss: 0.7175 - accuracy: 0.4971 - 159ms/epoch - 2ms/step\n",
      "Epoch 147/1000\n",
      "102/102 - 0s - loss: 0.7169 - accuracy: 0.5155 - 159ms/epoch - 2ms/step\n",
      "Epoch 148/1000\n",
      "102/102 - 0s - loss: 0.7161 - accuracy: 0.5026 - 157ms/epoch - 2ms/step\n",
      "Epoch 149/1000\n",
      "102/102 - 0s - loss: 0.7160 - accuracy: 0.5091 - 157ms/epoch - 2ms/step\n",
      "Epoch 150/1000\n",
      "102/102 - 0s - loss: 0.7158 - accuracy: 0.4955 - 157ms/epoch - 2ms/step\n",
      "Epoch 151/1000\n",
      "102/102 - 0s - loss: 0.7168 - accuracy: 0.5100 - 159ms/epoch - 2ms/step\n",
      "Epoch 152/1000\n",
      "102/102 - 0s - loss: 0.7169 - accuracy: 0.5038 - 160ms/epoch - 2ms/step\n",
      "Epoch 153/1000\n",
      "102/102 - 0s - loss: 0.7185 - accuracy: 0.5029 - 158ms/epoch - 2ms/step\n",
      "Epoch 154/1000\n",
      "102/102 - 0s - loss: 0.7160 - accuracy: 0.5032 - 158ms/epoch - 2ms/step\n",
      "Epoch 155/1000\n",
      "102/102 - 0s - loss: 0.7153 - accuracy: 0.5109 - 162ms/epoch - 2ms/step\n",
      "Epoch 156/1000\n",
      "102/102 - 0s - loss: 0.7164 - accuracy: 0.4922 - 161ms/epoch - 2ms/step\n",
      "Epoch 157/1000\n",
      "102/102 - 0s - loss: 0.7165 - accuracy: 0.5152 - 160ms/epoch - 2ms/step\n",
      "Epoch 158/1000\n",
      "102/102 - 0s - loss: 0.7152 - accuracy: 0.5118 - 151ms/epoch - 1ms/step\n",
      "Epoch 159/1000\n",
      "102/102 - 0s - loss: 0.7147 - accuracy: 0.5315 - 158ms/epoch - 2ms/step\n",
      "Epoch 160/1000\n",
      "102/102 - 0s - loss: 0.7156 - accuracy: 0.5152 - 198ms/epoch - 2ms/step\n",
      "Epoch 161/1000\n",
      "102/102 - 0s - loss: 0.7178 - accuracy: 0.5038 - 140ms/epoch - 1ms/step\n",
      "Epoch 162/1000\n",
      "102/102 - 0s - loss: 0.7156 - accuracy: 0.5008 - 157ms/epoch - 2ms/step\n",
      "Epoch 163/1000\n",
      "102/102 - 0s - loss: 0.7169 - accuracy: 0.4962 - 152ms/epoch - 1ms/step\n",
      "Epoch 164/1000\n",
      "102/102 - 0s - loss: 0.7164 - accuracy: 0.5005 - 159ms/epoch - 2ms/step\n",
      "Epoch 165/1000\n",
      "102/102 - 0s - loss: 0.7149 - accuracy: 0.5134 - 153ms/epoch - 2ms/step\n",
      "Epoch 166/1000\n",
      "102/102 - 0s - loss: 0.7160 - accuracy: 0.5128 - 157ms/epoch - 2ms/step\n",
      "Epoch 167/1000\n",
      "102/102 - 0s - loss: 0.7161 - accuracy: 0.5008 - 159ms/epoch - 2ms/step\n",
      "Epoch 168/1000\n",
      "102/102 - 0s - loss: 0.7152 - accuracy: 0.5094 - 157ms/epoch - 2ms/step\n",
      "Epoch 169/1000\n",
      "102/102 - 0s - loss: 0.7165 - accuracy: 0.5048 - 156ms/epoch - 2ms/step\n",
      "Epoch 170/1000\n",
      "102/102 - 0s - loss: 0.7158 - accuracy: 0.4983 - 156ms/epoch - 2ms/step\n",
      "Epoch 171/1000\n",
      "102/102 - 0s - loss: 0.7158 - accuracy: 0.5008 - 163ms/epoch - 2ms/step\n",
      "Epoch 172/1000\n",
      "102/102 - 0s - loss: 0.7162 - accuracy: 0.5078 - 162ms/epoch - 2ms/step\n",
      "Epoch 173/1000\n",
      "102/102 - 0s - loss: 0.7156 - accuracy: 0.5159 - 158ms/epoch - 2ms/step\n",
      "Epoch 174/1000\n",
      "102/102 - 0s - loss: 0.7157 - accuracy: 0.5020 - 160ms/epoch - 2ms/step\n",
      "Epoch 175/1000\n",
      "102/102 - 0s - loss: 0.7156 - accuracy: 0.5082 - 163ms/epoch - 2ms/step\n",
      "Epoch 176/1000\n",
      "102/102 - 0s - loss: 0.7152 - accuracy: 0.5054 - 158ms/epoch - 2ms/step\n",
      "Epoch 177/1000\n",
      "102/102 - 0s - loss: 0.7151 - accuracy: 0.5066 - 159ms/epoch - 2ms/step\n",
      "Epoch 178/1000\n",
      "102/102 - 0s - loss: 0.7171 - accuracy: 0.4922 - 160ms/epoch - 2ms/step\n",
      "Epoch 179/1000\n",
      "102/102 - 0s - loss: 0.7162 - accuracy: 0.5002 - 160ms/epoch - 2ms/step\n",
      "Epoch 180/1000\n",
      "102/102 - 0s - loss: 0.7151 - accuracy: 0.5183 - 161ms/epoch - 2ms/step\n",
      "Epoch 181/1000\n",
      "102/102 - 0s - loss: 0.7157 - accuracy: 0.5048 - 163ms/epoch - 2ms/step\n",
      "Epoch 182/1000\n",
      "102/102 - 0s - loss: 0.7151 - accuracy: 0.5106 - 157ms/epoch - 2ms/step\n",
      "Epoch 183/1000\n",
      "102/102 - 0s - loss: 0.7161 - accuracy: 0.5085 - 155ms/epoch - 2ms/step\n",
      "Epoch 184/1000\n",
      "102/102 - 0s - loss: 0.7155 - accuracy: 0.5112 - 154ms/epoch - 2ms/step\n",
      "Epoch 185/1000\n",
      "102/102 - 0s - loss: 0.7160 - accuracy: 0.5078 - 156ms/epoch - 2ms/step\n",
      "Epoch 186/1000\n",
      "102/102 - 0s - loss: 0.7147 - accuracy: 0.5131 - 155ms/epoch - 2ms/step\n",
      "Epoch 187/1000\n",
      "102/102 - 0s - loss: 0.7161 - accuracy: 0.5091 - 157ms/epoch - 2ms/step\n",
      "Epoch 188/1000\n",
      "102/102 - 0s - loss: 0.7152 - accuracy: 0.5115 - 154ms/epoch - 2ms/step\n",
      "Epoch 189/1000\n",
      "102/102 - 0s - loss: 0.7155 - accuracy: 0.5115 - 157ms/epoch - 2ms/step\n",
      "Epoch 190/1000\n",
      "102/102 - 0s - loss: 0.7162 - accuracy: 0.5137 - 168ms/epoch - 2ms/step\n",
      "Epoch 191/1000\n",
      "102/102 - 0s - loss: 0.7156 - accuracy: 0.4952 - 157ms/epoch - 2ms/step\n",
      "Epoch 192/1000\n",
      "102/102 - 0s - loss: 0.7157 - accuracy: 0.5134 - 157ms/epoch - 2ms/step\n",
      "Epoch 193/1000\n",
      "102/102 - 0s - loss: 0.7168 - accuracy: 0.4992 - 159ms/epoch - 2ms/step\n",
      "Epoch 194/1000\n",
      "102/102 - 0s - loss: 0.7144 - accuracy: 0.5152 - 159ms/epoch - 2ms/step\n",
      "Epoch 195/1000\n",
      "102/102 - 0s - loss: 0.7155 - accuracy: 0.4971 - 155ms/epoch - 2ms/step\n",
      "Epoch 196/1000\n",
      "102/102 - 0s - loss: 0.7156 - accuracy: 0.5066 - 193ms/epoch - 2ms/step\n",
      "Epoch 197/1000\n",
      "102/102 - 0s - loss: 0.7154 - accuracy: 0.5082 - 150ms/epoch - 1ms/step\n",
      "Epoch 198/1000\n",
      "102/102 - 0s - loss: 0.7154 - accuracy: 0.5023 - 162ms/epoch - 2ms/step\n",
      "Epoch 199/1000\n",
      "102/102 - 0s - loss: 0.7156 - accuracy: 0.5023 - 159ms/epoch - 2ms/step\n",
      "Epoch 200/1000\n",
      "102/102 - 0s - loss: 0.7162 - accuracy: 0.5069 - 155ms/epoch - 2ms/step\n",
      "Epoch 201/1000\n",
      "102/102 - 0s - loss: 0.7166 - accuracy: 0.4980 - 173ms/epoch - 2ms/step\n",
      "Epoch 202/1000\n",
      "102/102 - 0s - loss: 0.7154 - accuracy: 0.4962 - 166ms/epoch - 2ms/step\n",
      "Epoch 203/1000\n",
      "102/102 - 0s - loss: 0.7152 - accuracy: 0.5134 - 169ms/epoch - 2ms/step\n",
      "Epoch 204/1000\n",
      "102/102 - 0s - loss: 0.7175 - accuracy: 0.4955 - 169ms/epoch - 2ms/step\n",
      "Epoch 205/1000\n",
      "102/102 - 0s - loss: 0.7161 - accuracy: 0.5082 - 170ms/epoch - 2ms/step\n",
      "Epoch 206/1000\n",
      "102/102 - 0s - loss: 0.7159 - accuracy: 0.5020 - 165ms/epoch - 2ms/step\n",
      "Epoch 207/1000\n",
      "102/102 - 0s - loss: 0.7161 - accuracy: 0.5075 - 162ms/epoch - 2ms/step\n",
      "Epoch 208/1000\n",
      "102/102 - 0s - loss: 0.7139 - accuracy: 0.5183 - 164ms/epoch - 2ms/step\n",
      "Epoch 209/1000\n",
      "102/102 - 0s - loss: 0.7158 - accuracy: 0.5011 - 163ms/epoch - 2ms/step\n",
      "Epoch 210/1000\n",
      "102/102 - 0s - loss: 0.7151 - accuracy: 0.5195 - 167ms/epoch - 2ms/step\n",
      "Epoch 211/1000\n",
      "102/102 - 0s - loss: 0.7153 - accuracy: 0.5103 - 164ms/epoch - 2ms/step\n",
      "Epoch 212/1000\n",
      "102/102 - 0s - loss: 0.7152 - accuracy: 0.5091 - 168ms/epoch - 2ms/step\n",
      "Epoch 213/1000\n",
      "102/102 - 0s - loss: 0.7164 - accuracy: 0.5115 - 164ms/epoch - 2ms/step\n",
      "Epoch 214/1000\n",
      "102/102 - 0s - loss: 0.7159 - accuracy: 0.5088 - 162ms/epoch - 2ms/step\n",
      "Epoch 215/1000\n",
      "102/102 - 0s - loss: 0.7156 - accuracy: 0.4940 - 164ms/epoch - 2ms/step\n",
      "Epoch 216/1000\n",
      "102/102 - 0s - loss: 0.7152 - accuracy: 0.5069 - 162ms/epoch - 2ms/step\n",
      "Epoch 217/1000\n",
      "102/102 - 0s - loss: 0.7143 - accuracy: 0.5239 - 166ms/epoch - 2ms/step\n",
      "Epoch 218/1000\n",
      "102/102 - 0s - loss: 0.7149 - accuracy: 0.5137 - 162ms/epoch - 2ms/step\n",
      "Epoch 219/1000\n",
      "102/102 - 0s - loss: 0.7165 - accuracy: 0.5162 - 165ms/epoch - 2ms/step\n",
      "Epoch 220/1000\n",
      "102/102 - 0s - loss: 0.7163 - accuracy: 0.5118 - 166ms/epoch - 2ms/step\n",
      "Epoch 221/1000\n",
      "102/102 - 0s - loss: 0.7177 - accuracy: 0.5014 - 161ms/epoch - 2ms/step\n",
      "Epoch 222/1000\n",
      "102/102 - 0s - loss: 0.7153 - accuracy: 0.5202 - 165ms/epoch - 2ms/step\n",
      "Epoch 223/1000\n",
      "102/102 - 0s - loss: 0.7151 - accuracy: 0.5097 - 161ms/epoch - 2ms/step\n",
      "Epoch 224/1000\n",
      "102/102 - 0s - loss: 0.7154 - accuracy: 0.5063 - 163ms/epoch - 2ms/step\n",
      "Epoch 225/1000\n",
      "102/102 - 0s - loss: 0.7155 - accuracy: 0.5146 - 165ms/epoch - 2ms/step\n",
      "Epoch 226/1000\n",
      "102/102 - 0s - loss: 0.7150 - accuracy: 0.5174 - 164ms/epoch - 2ms/step\n",
      "Epoch 227/1000\n",
      "102/102 - 0s - loss: 0.7150 - accuracy: 0.5118 - 165ms/epoch - 2ms/step\n",
      "Epoch 228/1000\n",
      "102/102 - 0s - loss: 0.7156 - accuracy: 0.5109 - 169ms/epoch - 2ms/step\n",
      "Epoch 229/1000\n",
      "102/102 - 0s - loss: 0.7168 - accuracy: 0.5023 - 163ms/epoch - 2ms/step\n",
      "Epoch 230/1000\n",
      "102/102 - 0s - loss: 0.7164 - accuracy: 0.5091 - 160ms/epoch - 2ms/step\n",
      "Epoch 231/1000\n",
      "102/102 - 0s - loss: 0.7155 - accuracy: 0.5137 - 162ms/epoch - 2ms/step\n",
      "Epoch 232/1000\n",
      "102/102 - 0s - loss: 0.7153 - accuracy: 0.5002 - 165ms/epoch - 2ms/step\n",
      "Epoch 233/1000\n",
      "102/102 - 0s - loss: 0.7139 - accuracy: 0.5223 - 159ms/epoch - 2ms/step\n",
      "Epoch 234/1000\n",
      "102/102 - 0s - loss: 0.7160 - accuracy: 0.5082 - 209ms/epoch - 2ms/step\n",
      "Epoch 235/1000\n",
      "102/102 - 0s - loss: 0.7158 - accuracy: 0.4955 - 144ms/epoch - 1ms/step\n",
      "Epoch 236/1000\n",
      "102/102 - 0s - loss: 0.7155 - accuracy: 0.5042 - 153ms/epoch - 2ms/step\n",
      "Epoch 237/1000\n",
      "102/102 - 0s - loss: 0.7150 - accuracy: 0.5122 - 164ms/epoch - 2ms/step\n",
      "Epoch 238/1000\n",
      "102/102 - 0s - loss: 0.7153 - accuracy: 0.5082 - 155ms/epoch - 2ms/step\n",
      "Epoch 239/1000\n",
      "102/102 - 0s - loss: 0.7162 - accuracy: 0.5048 - 153ms/epoch - 1ms/step\n",
      "Epoch 240/1000\n",
      "102/102 - 0s - loss: 0.7154 - accuracy: 0.5042 - 155ms/epoch - 2ms/step\n",
      "Epoch 241/1000\n",
      "102/102 - 0s - loss: 0.7156 - accuracy: 0.5128 - 162ms/epoch - 2ms/step\n",
      "Epoch 242/1000\n",
      "102/102 - 0s - loss: 0.7157 - accuracy: 0.5094 - 160ms/epoch - 2ms/step\n",
      "Epoch 243/1000\n",
      "102/102 - 0s - loss: 0.7150 - accuracy: 0.5125 - 160ms/epoch - 2ms/step\n",
      "Epoch 244/1000\n",
      "102/102 - 0s - loss: 0.7154 - accuracy: 0.5140 - 159ms/epoch - 2ms/step\n",
      "Epoch 245/1000\n",
      "102/102 - 0s - loss: 0.7167 - accuracy: 0.4998 - 158ms/epoch - 2ms/step\n",
      "Epoch 246/1000\n",
      "102/102 - 0s - loss: 0.7161 - accuracy: 0.5078 - 159ms/epoch - 2ms/step\n",
      "Epoch 247/1000\n",
      "102/102 - 0s - loss: 0.7158 - accuracy: 0.5140 - 156ms/epoch - 2ms/step\n",
      "Epoch 248/1000\n",
      "102/102 - 0s - loss: 0.7158 - accuracy: 0.5051 - 162ms/epoch - 2ms/step\n",
      "Epoch 249/1000\n",
      "102/102 - 0s - loss: 0.7150 - accuracy: 0.5143 - 160ms/epoch - 2ms/step\n",
      "Epoch 250/1000\n",
      "102/102 - 0s - loss: 0.7162 - accuracy: 0.5112 - 159ms/epoch - 2ms/step\n",
      "Epoch 251/1000\n",
      "102/102 - 0s - loss: 0.7155 - accuracy: 0.5026 - 159ms/epoch - 2ms/step\n",
      "Epoch 252/1000\n",
      "102/102 - 0s - loss: 0.7162 - accuracy: 0.5048 - 160ms/epoch - 2ms/step\n",
      "Epoch 253/1000\n",
      "102/102 - 0s - loss: 0.7150 - accuracy: 0.5072 - 168ms/epoch - 2ms/step\n",
      "Epoch 254/1000\n",
      "102/102 - 0s - loss: 0.7158 - accuracy: 0.5085 - 171ms/epoch - 2ms/step\n",
      "Epoch 255/1000\n",
      "102/102 - 0s - loss: 0.7153 - accuracy: 0.4983 - 181ms/epoch - 2ms/step\n",
      "Epoch 256/1000\n",
      "102/102 - 0s - loss: 0.7150 - accuracy: 0.5026 - 154ms/epoch - 2ms/step\n",
      "Epoch 257/1000\n",
      "102/102 - 0s - loss: 0.7157 - accuracy: 0.5171 - 174ms/epoch - 2ms/step\n",
      "Epoch 258/1000\n",
      "102/102 - 0s - loss: 0.7153 - accuracy: 0.5146 - 148ms/epoch - 1ms/step\n",
      "Epoch 259/1000\n",
      "102/102 - 0s - loss: 0.7163 - accuracy: 0.4946 - 180ms/epoch - 2ms/step\n",
      "Epoch 260/1000\n",
      "102/102 - 0s - loss: 0.7148 - accuracy: 0.5180 - 156ms/epoch - 2ms/step\n",
      "Epoch 261/1000\n",
      "102/102 - 0s - loss: 0.7153 - accuracy: 0.5122 - 158ms/epoch - 2ms/step\n",
      "Epoch 262/1000\n",
      "102/102 - 0s - loss: 0.7140 - accuracy: 0.5162 - 173ms/epoch - 2ms/step\n",
      "Epoch 263/1000\n",
      "102/102 - 0s - loss: 0.7157 - accuracy: 0.5125 - 139ms/epoch - 1ms/step\n",
      "Epoch 264/1000\n",
      "102/102 - 0s - loss: 0.7152 - accuracy: 0.5048 - 157ms/epoch - 2ms/step\n",
      "Epoch 265/1000\n",
      "102/102 - 0s - loss: 0.7155 - accuracy: 0.5048 - 186ms/epoch - 2ms/step\n",
      "Epoch 266/1000\n",
      "102/102 - 0s - loss: 0.7151 - accuracy: 0.5140 - 169ms/epoch - 2ms/step\n",
      "Epoch 267/1000\n",
      "102/102 - 0s - loss: 0.7152 - accuracy: 0.4968 - 177ms/epoch - 2ms/step\n",
      "Epoch 268/1000\n",
      "102/102 - 0s - loss: 0.7148 - accuracy: 0.5131 - 174ms/epoch - 2ms/step\n",
      "Epoch 269/1000\n",
      "102/102 - 0s - loss: 0.7152 - accuracy: 0.5115 - 146ms/epoch - 1ms/step\n",
      "Epoch 270/1000\n",
      "102/102 - 0s - loss: 0.7149 - accuracy: 0.4931 - 205ms/epoch - 2ms/step\n",
      "Epoch 271/1000\n",
      "102/102 - 0s - loss: 0.7157 - accuracy: 0.5011 - 187ms/epoch - 2ms/step\n",
      "Epoch 272/1000\n",
      "102/102 - 0s - loss: 0.7148 - accuracy: 0.5100 - 145ms/epoch - 1ms/step\n",
      "Epoch 273/1000\n",
      "102/102 - 0s - loss: 0.7154 - accuracy: 0.5017 - 156ms/epoch - 2ms/step\n",
      "Epoch 274/1000\n",
      "102/102 - 0s - loss: 0.7146 - accuracy: 0.5155 - 155ms/epoch - 2ms/step\n",
      "Epoch 275/1000\n",
      "102/102 - 0s - loss: 0.7148 - accuracy: 0.5057 - 198ms/epoch - 2ms/step\n",
      "Epoch 276/1000\n",
      "102/102 - 0s - loss: 0.7146 - accuracy: 0.5137 - 154ms/epoch - 2ms/step\n",
      "Epoch 277/1000\n",
      "102/102 - 0s - loss: 0.7156 - accuracy: 0.5088 - 156ms/epoch - 2ms/step\n",
      "Epoch 278/1000\n",
      "102/102 - 0s - loss: 0.7146 - accuracy: 0.5143 - 199ms/epoch - 2ms/step\n",
      "Epoch 279/1000\n",
      "102/102 - 0s - loss: 0.7150 - accuracy: 0.5112 - 178ms/epoch - 2ms/step\n",
      "Epoch 280/1000\n",
      "102/102 - 0s - loss: 0.7147 - accuracy: 0.5085 - 176ms/epoch - 2ms/step\n",
      "Epoch 281/1000\n",
      "102/102 - 0s - loss: 0.7146 - accuracy: 0.5069 - 149ms/epoch - 1ms/step\n",
      "Epoch 282/1000\n",
      "102/102 - 0s - loss: 0.7165 - accuracy: 0.5085 - 154ms/epoch - 2ms/step\n",
      "Epoch 283/1000\n",
      "102/102 - 0s - loss: 0.7146 - accuracy: 0.5134 - 165ms/epoch - 2ms/step\n",
      "Epoch 284/1000\n",
      "102/102 - 0s - loss: 0.7147 - accuracy: 0.5109 - 142ms/epoch - 1ms/step\n",
      "Epoch 285/1000\n",
      "102/102 - 0s - loss: 0.7142 - accuracy: 0.5174 - 176ms/epoch - 2ms/step\n",
      "Epoch 286/1000\n",
      "102/102 - 0s - loss: 0.7149 - accuracy: 0.5078 - 202ms/epoch - 2ms/step\n",
      "Epoch 287/1000\n",
      "102/102 - 0s - loss: 0.7150 - accuracy: 0.5085 - 151ms/epoch - 1ms/step\n",
      "Epoch 288/1000\n",
      "102/102 - 0s - loss: 0.7143 - accuracy: 0.5183 - 153ms/epoch - 1ms/step\n",
      "Epoch 289/1000\n",
      "102/102 - 0s - loss: 0.7154 - accuracy: 0.5066 - 213ms/epoch - 2ms/step\n",
      "Epoch 290/1000\n",
      "102/102 - 0s - loss: 0.7149 - accuracy: 0.5075 - 210ms/epoch - 2ms/step\n",
      "Epoch 291/1000\n",
      "102/102 - 0s - loss: 0.7151 - accuracy: 0.5045 - 141ms/epoch - 1ms/step\n",
      "Epoch 292/1000\n",
      "102/102 - 0s - loss: 0.7160 - accuracy: 0.5097 - 171ms/epoch - 2ms/step\n",
      "Epoch 293/1000\n",
      "102/102 - 0s - loss: 0.7148 - accuracy: 0.5103 - 170ms/epoch - 2ms/step\n",
      "Epoch 294/1000\n",
      "102/102 - 0s - loss: 0.7148 - accuracy: 0.5146 - 162ms/epoch - 2ms/step\n",
      "Epoch 295/1000\n",
      "102/102 - 0s - loss: 0.7146 - accuracy: 0.5125 - 176ms/epoch - 2ms/step\n",
      "Epoch 296/1000\n",
      "102/102 - 0s - loss: 0.7145 - accuracy: 0.5118 - 145ms/epoch - 1ms/step\n",
      "Epoch 297/1000\n",
      "102/102 - 0s - loss: 0.7151 - accuracy: 0.5091 - 160ms/epoch - 2ms/step\n",
      "Epoch 298/1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m a \u001b[39m=\u001b[39m NN(deon,keyon,lag,\u001b[39m3\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[39], line 26\u001b[0m, in \u001b[0;36mNN\u001b[1;34m(x, y, input_dim, out_dim)\u001b[0m\n\u001b[0;32m     23\u001b[0m optimizer\u001b[39m=\u001b[39mAdam()\n\u001b[0;32m     24\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39moptimizer, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m,metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 26\u001b[0m model\u001b[39m.\u001b[39;49mfit(x, y, epochs\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[0;32m     27\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1641\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1639\u001b[0m callbacks\u001b[39m.\u001b[39mon_epoch_begin(epoch)\n\u001b[0;32m   1640\u001b[0m \u001b[39mwith\u001b[39;00m data_handler\u001b[39m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m-> 1641\u001b[0m     \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n\u001b[0;32m   1642\u001b[0m         \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1643\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1644\u001b[0m             epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1647\u001b[0m             _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1648\u001b[0m         ):\n\u001b[0;32m   1649\u001b[0m             callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\data_adapter.py:1371\u001b[0m, in \u001b[0;36mDataHandler.steps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1369\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_insufficient_data:  \u001b[39m# Set by `catch_stop_iteration`.\u001b[39;00m\n\u001b[0;32m   1370\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1371\u001b[0m original_spe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution\u001b[39m.\u001b[39;49mnumpy()\u001b[39m.\u001b[39mitem()\n\u001b[0;32m   1372\u001b[0m can_run_full_execution \u001b[39m=\u001b[39m (\n\u001b[0;32m   1373\u001b[0m     original_spe \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1374\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_steps \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1375\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_steps \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m original_spe\n\u001b[0;32m   1376\u001b[0m )\n\u001b[0;32m   1378\u001b[0m \u001b[39mif\u001b[39;00m can_run_full_execution:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:639\u001b[0m, in \u001b[0;36mBaseResourceVariable.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnumpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    638\u001b[0m   \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 639\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread_value()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m    640\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    641\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mnumpy() is only available when eager execution is enabled.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:727\u001b[0m, in \u001b[0;36mBaseResourceVariable.read_value\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[39m\"\"\"Constructs an op which reads the value of this variable.\u001b[39;00m\n\u001b[0;32m    719\u001b[0m \n\u001b[0;32m    720\u001b[0m \u001b[39mShould be used when there are multiple reads, or when it is desirable to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[39m  The value of the variable.\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(\u001b[39m\"\u001b[39m\u001b[39mRead\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 727\u001b[0m   value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_variable_op()\n\u001b[0;32m    728\u001b[0m \u001b[39m# Return an identity so it can get placed on whatever device the context\u001b[39;00m\n\u001b[0;32m    729\u001b[0m \u001b[39m# specifies instead of the device where the variable is.\u001b[39;00m\n\u001b[0;32m    730\u001b[0m \u001b[39mreturn\u001b[39;00m array_ops\u001b[39m.\u001b[39midentity(value)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:706\u001b[0m, in \u001b[0;36mBaseResourceVariable._read_variable_op\u001b[1;34m(self, no_copy)\u001b[0m\n\u001b[0;32m    704\u001b[0m       result \u001b[39m=\u001b[39m read_and_set_handle(no_copy)\n\u001b[0;32m    705\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m   result \u001b[39m=\u001b[39m read_and_set_handle(no_copy)\n\u001b[0;32m    708\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m    709\u001b[0m   \u001b[39m# Note that if a control flow context is active the input of the read op\u001b[39;00m\n\u001b[0;32m    710\u001b[0m   \u001b[39m# might not actually be the handle. This line bypasses it.\u001b[39;00m\n\u001b[0;32m    711\u001b[0m   tape\u001b[39m.\u001b[39mrecord_operation(\n\u001b[0;32m    712\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mReadVariableOp\u001b[39m\u001b[39m\"\u001b[39m, [result], [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle],\n\u001b[0;32m    713\u001b[0m       backward_function\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: [x],\n\u001b[0;32m    714\u001b[0m       forward_function\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: [x])\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:696\u001b[0m, in \u001b[0;36mBaseResourceVariable._read_variable_op.<locals>.read_and_set_handle\u001b[1;34m(no_copy)\u001b[0m\n\u001b[0;32m    694\u001b[0m \u001b[39mif\u001b[39;00m no_copy \u001b[39mand\u001b[39;00m forward_compat\u001b[39m.\u001b[39mforward_compatible(\u001b[39m2022\u001b[39m, \u001b[39m5\u001b[39m, \u001b[39m3\u001b[39m):\n\u001b[0;32m    695\u001b[0m   gen_resource_variable_ops\u001b[39m.\u001b[39mdisable_copy_on_read(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle)\n\u001b[1;32m--> 696\u001b[0m result \u001b[39m=\u001b[39m gen_resource_variable_ops\u001b[39m.\u001b[39;49mread_variable_op(\n\u001b[0;32m    697\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dtype)\n\u001b[0;32m    698\u001b[0m _maybe_set_handle_data(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dtype, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle, result)\n\u001b[0;32m    699\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py:580\u001b[0m, in \u001b[0;36mread_variable_op\u001b[1;34m(resource, dtype, name)\u001b[0m\n\u001b[0;32m    578\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m    579\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 580\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m    581\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mReadVariableOp\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, resource, \u001b[39m\"\u001b[39;49m\u001b[39mdtype\u001b[39;49m\u001b[39m\"\u001b[39;49m, dtype)\n\u001b[0;32m    582\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m    583\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "a = NN(deon,keyon,lag,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.50590086, 0.00244201, 0.49165714],\n",
       "       [0.5059009 , 0.00244214, 0.49165696],\n",
       "       [0.5058987 , 0.00244204, 0.49165922],\n",
       "       ...,\n",
       "       [0.5059011 , 0.00244218, 0.49165675],\n",
       "       [0.5058996 , 0.00244185, 0.4916585 ],\n",
       "       [0.50589937, 0.00244192, 0.4916587 ]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d=a.predict(dete)\n",
    "d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a42ccb73e7d9bfdf27e036f1d2b8b681e55fc0743cc5586bc2474d4a60f4b886"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
